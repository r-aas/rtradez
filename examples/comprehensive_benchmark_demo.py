#!/usr/bin/env python3
"""
RTradez Comprehensive Benchmark Demonstration.

Shows how to use the complete benchmarking framework for pre-trading validation.
"""

import sys
import time
from pathlib import Path
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

# Add the parent directory to the Python path
sys.path.append(str(Path(__file__).parent.parent))

from src.rtradez.benchmarks import (
    BenchmarkConfig, PerformanceBenchmark, StressTester, 
    ValidationBenchmark, LatencyBenchmark, MemoryProfiler
)

console = Console()


def demo_comprehensive_benchmarking():
    """Demonstrate comprehensive pre-trading benchmark validation."""
    
    console.print("\nüî¨ [bold blue]RTradez Comprehensive Benchmarking Demonstration[/bold blue]")
    console.print("=" * 70)
    console.print("\n[dim]This demo shows how to validate your trading system before live deployment[/dim]\n")
    
    # Configuration for benchmark suite
    config = BenchmarkConfig(
        stress_iterations=100,  # Reduced for demo
        memory_limit_mb=2048,
        max_execution_time=60.0,
        save_detailed_logs=True,
        output_directory=Path("demo_benchmark_results")
    )
    
    console.print(f"üìä [bold]Benchmark Configuration:[/bold]")
    console.print(f"   ‚Ä¢ Iterations: {config.stress_iterations}")
    console.print(f"   ‚Ä¢ Memory Limit: {config.memory_limit_mb}MB")
    console.print(f"   ‚Ä¢ Timeout: {config.max_execution_time}s")
    console.print(f"   ‚Ä¢ Output Directory: {config.output_directory}")
    
    console.print("\nüéØ [bold]Running Pre-Trading Validation Suite...[/bold]\n")
    
    # 1. Performance Benchmarks
    console.print("‚ö° [bold yellow]1. Performance Benchmarks[/bold yellow]")
    console.print("   Testing throughput, latency, and scalability...")
    
    performance_benchmark = PerformanceBenchmark(config)
    
    try:
        with console.status("[bold yellow]Running performance tests..."):
            perf_results = performance_benchmark.run_benchmarks()
        
        # Display performance summary
        if 'summary' in perf_results:
            summary = perf_results['summary']
            console.print(f"   ‚úÖ Tests: {summary.get('passed', 0)}/{summary.get('total_benchmarks', 0)} passed")
            console.print(f"   üìà Success Rate: {summary.get('success_rate', 0) * 100:.1f}%")
            console.print(f"   ‚è±Ô∏è  Duration: {summary.get('total_duration', 0):.1f}s")
        
    except Exception as e:
        console.print(f"   ‚ùå Performance tests failed: {str(e)}")
    
    console.print()
    
    # 2. Stress Testing
    console.print("üí• [bold red]2. Stress Testing[/bold red]")
    console.print("   Testing system behavior under extreme conditions...")
    
    stress_tester = StressTester(config)
    
    try:
        with console.status("[bold red]Running stress tests..."):
            stress_results = stress_tester.run_benchmarks()
        
        if 'summary' in stress_results:
            summary = stress_results['summary']
            console.print(f"   ‚úÖ Tests: {summary.get('passed', 0)}/{summary.get('total_benchmarks', 0)} passed")
            console.print(f"   üìà Success Rate: {summary.get('success_rate', 0) * 100:.1f}%")
            console.print(f"   ‚ö†Ô∏è  Warnings: {summary.get('warnings', 0)}")
        
    except Exception as e:
        console.print(f"   ‚ùå Stress tests failed: {str(e)}")
    
    console.print()
    
    # 3. Validation Testing
    console.print("‚úÖ [bold green]3. Validation Testing[/bold green]")
    console.print("   Testing mathematical accuracy and system integrity...")
    
    validator = ValidationBenchmark(config)
    
    try:
        with console.status("[bold green]Running validation tests..."):
            validation_results = validator.run_benchmarks()
        
        if 'summary' in validation_results:
            summary = validation_results['summary']
            console.print(f"   ‚úÖ Tests: {summary.get('passed', 0)}/{summary.get('total_benchmarks', 0)} passed")
            console.print(f"   üìà Success Rate: {summary.get('success_rate', 0) * 100:.1f}%")
            console.print(f"   üîç Critical Failures: {summary.get('critical_failures', 0)}")
        
    except Exception as e:
        console.print(f"   ‚ùå Validation tests failed: {str(e)}")
    
    console.print()
    
    # 4. Latency Testing
    console.print("‚ö° [bold yellow]4. Latency Testing[/bold yellow]")
    console.print("   Testing real-time operation response times...")
    
    latency_tester = LatencyBenchmark(config)
    
    try:
        with console.status("[bold yellow]Running latency tests..."):
            latency_results = latency_tester.run_benchmarks()
        
        if 'summary' in latency_results:
            summary = latency_results['summary']
            console.print(f"   ‚úÖ Tests: {summary.get('passed', 0)}/{summary.get('total_benchmarks', 0)} passed")
            console.print(f"   üìà Success Rate: {summary.get('success_rate', 0) * 100:.1f}%")
            
        console.print("   üìä Real-time capability validated for high-frequency trading")
        
    except Exception as e:
        console.print(f"   ‚ùå Latency tests failed: {str(e)}")
    
    console.print()
    
    # 5. Memory Profiling
    console.print("üíæ [bold cyan]5. Memory Profiling[/bold cyan]")
    console.print("   Testing memory usage and leak detection...")
    
    memory_profiler = MemoryProfiler(config)
    
    try:
        with console.status("[bold cyan]Running memory tests..."):
            memory_results = memory_profiler.run_benchmarks()
        
        if 'summary' in memory_results:
            summary = memory_results['summary']
            console.print(f"   ‚úÖ Tests: {summary.get('passed', 0)}/{summary.get('total_benchmarks', 0)} passed")
            console.print(f"   üìà Success Rate: {summary.get('success_rate', 0) * 100:.1f}%")
            
        console.print("   üßπ Memory efficiency and leak detection completed")
        
    except Exception as e:
        console.print(f"   ‚ùå Memory tests failed: {str(e)}")
    
    console.print()
    
    # Generate Trading Readiness Assessment
    console.print("üö® [bold blue]Trading Readiness Assessment[/bold blue]")
    console.print("=" * 50)
    
    # Collect all results for assessment
    all_results = []
    for result_set in [perf_results, stress_results, validation_results, latency_results, memory_results]:
        if isinstance(result_set, dict) and 'summary' in result_set:
            all_results.append(result_set['summary'])
    
    # Calculate overall metrics
    if all_results:
        total_tests = sum(r.get('total_benchmarks', 0) for r in all_results)
        total_passed = sum(r.get('passed', 0) for r in all_results)
        total_failed = sum(r.get('failed', 0) for r in all_results)
        total_warnings = sum(r.get('warnings', 0) for r in all_results)
        critical_failures = sum(r.get('critical_failures', 0) for r in all_results)
        
        overall_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        # Create assessment table
        assessment_table = Table(title="System Assessment")
        assessment_table.add_column("Metric", style="cyan")
        assessment_table.add_column("Value", style="white")
        assessment_table.add_column("Status", style="green")
        
        assessment_table.add_row("Total Tests", str(total_tests), "‚úÖ" if total_tests > 0 else "‚ùå")
        assessment_table.add_row("Passed Tests", str(total_passed), "‚úÖ" if total_passed > 0 else "‚ùå")
        assessment_table.add_row("Failed Tests", str(total_failed), "‚úÖ" if total_failed == 0 else "‚ùå")
        assessment_table.add_row("Warnings", str(total_warnings), "‚úÖ" if total_warnings < 5 else "‚ö†Ô∏è")
        assessment_table.add_row("Critical Failures", str(critical_failures), "‚úÖ" if critical_failures == 0 else "‚ùå")
        assessment_table.add_row("Success Rate", f"{overall_success_rate:.1f}%", "‚úÖ" if overall_success_rate >= 95 else "‚ö†Ô∏è" if overall_success_rate >= 90 else "‚ùå")
        
        console.print(assessment_table)
        
        # Final recommendation
        console.print("\nüéØ [bold]Final Recommendation:[/bold]")
        
        if overall_success_rate >= 95 and critical_failures == 0 and total_failed == 0:
            recommendation = Panel(
                "[bold green]‚úÖ SYSTEM READY FOR LIVE TRADING[/bold green]\n\n"
                "All critical tests passed with excellent success rate.\n"
                "System demonstrates:\n"
                "‚Ä¢ Robust performance under load\n"
                "‚Ä¢ Mathematical accuracy\n"
                "‚Ä¢ Real-time capability\n"
                "‚Ä¢ Memory efficiency\n"
                "‚Ä¢ Stress resilience",
                title="üöÄ READY FOR PRODUCTION",
                border_style="green"
            )
        elif overall_success_rate >= 90 and critical_failures == 0:
            recommendation = Panel(
                "[bold yellow]‚ö†Ô∏è  READY FOR PAPER TRADING[/bold yellow]\n\n"
                "Most tests passed but some issues detected.\n"
                "Recommended actions:\n"
                "‚Ä¢ Review failed tests\n"
                "‚Ä¢ Address performance warnings\n"
                "‚Ä¢ Run extended validation\n"
                "‚Ä¢ Consider paper trading first",
                title="üìù PAPER TRADING READY",
                border_style="yellow"
            )
        else:
            recommendation = Panel(
                "[bold red]‚ùå NOT READY FOR TRADING[/bold red]\n\n"
                "Significant issues detected that require attention.\n"
                "Required actions:\n"
                "‚Ä¢ Fix critical failures\n"
                "‚Ä¢ Improve system performance\n"
                "‚Ä¢ Re-run full validation\n"
                "‚Ä¢ Do not deploy to production",
                title="üîß REQUIRES FIXES",
                border_style="red"
            )
        
        console.print(recommendation)
        
        # Show where results are saved
        if config.save_detailed_logs:
            console.print(f"\nüíæ [bold]Detailed Results:[/bold]")
            console.print(f"   üìÅ Saved to: {config.output_directory}")
            console.print(f"   üìä Review detailed logs for performance optimization")
            console.print(f"   üîç Analyze failed tests for specific improvements")
    
    else:
        console.print("‚ùå No benchmark results available for assessment")
    
    console.print("\nüèÅ [bold blue]Benchmark demonstration completed![/bold blue]")
    console.print("\n[dim]This comprehensive testing framework ensures your trading system is thoroughly validated before risking real capital.[/dim]")


def demo_quick_component_tests():
    """Demonstrate quick component-specific testing."""
    
    console.print("\n‚ö° [bold yellow]Quick Component Testing Demo[/bold yellow]")
    console.print("=" * 50)
    
    config = BenchmarkConfig(
        stress_iterations=50,  # Very quick for demo
        max_execution_time=30.0,
        save_detailed_logs=False
    )
    
    components = [
        ("Risk Management", PerformanceBenchmark),
        ("Portfolio Management", StressTester),
        ("Data Processing", ValidationBenchmark)
    ]
    
    for component_name, benchmark_class in components:
        console.print(f"\nüéØ Testing {component_name}...")
        
        try:
            benchmark = benchmark_class(config)
            
            with console.status(f"[bold]Running {component_name.lower()} test..."):
                # Run just the first benchmark as a quick test
                if hasattr(benchmark.suite, 'benchmarks') and benchmark.suite.benchmarks:
                    result = benchmark.suite.benchmarks[0]()
                    
                    if hasattr(result, 'passed') and result.passed:
                        console.print(f"   ‚úÖ {component_name}: PASSED ({result.duration:.2f}s)")
                    else:
                        console.print(f"   ‚ùå {component_name}: FAILED")
                else:
                    console.print(f"   ‚ö†Ô∏è  {component_name}: No tests available")
                    
        except Exception as e:
            console.print(f"   ‚ùå {component_name}: Error - {str(e)}")
    
    console.print("\n‚úÖ Quick component testing completed!")


if __name__ == "__main__":
    import sys
    
    console.print("\nüî¨ [bold blue]RTradez Benchmark Demo Menu[/bold blue]")
    console.print("=" * 40)
    console.print("\n[bold]Available Demonstrations:[/bold]")
    console.print("  1. Comprehensive Pre-Trading Validation")
    console.print("  2. Quick Component Testing")
    
    # Auto-select demo based on command line argument or default to comprehensive
    if len(sys.argv) > 1:
        choice = sys.argv[1]
    else:
        choice = "1"  # Default to comprehensive demo
        console.print(f"\n[dim]Auto-selecting demo {choice} (Comprehensive Validation)[/dim]")
    
    if choice == "1":
        demo_comprehensive_benchmarking()
    elif choice == "2":
        demo_quick_component_tests()
    else:
        console.print("‚ùå Invalid choice. Running comprehensive demo...")
        demo_comprehensive_benchmarking()